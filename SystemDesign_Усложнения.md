<h3>Ожидание большой нагрузки</h3>

Изначально стоит сказать, что System Design достаточно обширная тема и раскрыть все даже в абзаце не получится, поэтому я лишь кратко опишу возможные пути решения данной проблемы. Сначала я опишу общий принцип, а затем постараюсь раскрыть на нашем конкретном примере.

Если сервис подвергается большой нагрузке, то общая `latency` падает и образуется высокая вероятность так называемой `high latency`. Проще говоря, баги и заторможенности в приложении. Поэтому нам необходимо увеличить наш `throughput`, чтобы система могла выдерживать больше нагрузки.

Самый очевидный способ решения - `caching`. Есть два вида кэширования: `write through cache` и `write back cache`. Первый обновляет данные в бд и кэше, а второй сначала только в кэше и потом асинхронно с бд. Предположим, что мы за первый вариант, так как во втором есть риск, что данные могут потеряться, если произойдет сбой. Например, мы будем использовать `Redis` в качестве нашего хранилища. Также установим `load balancer` между сервером и клиентом, чтобы тот распределял requests от пользователей на разные сервера. Также данный load balancer может сначала делать проверку в кэше на наличие похожего запроса от клиента и если таковой имеется в кэше, то `load balancer` вернет данные из кэша и не будет делать запрос к серверу(а как известно, скорость чтения из кэша быстрее).<br>
PS: `load balancer` может быть как перед сервером для распределения клиентов по серверам, так и после сервера для распределения запросов от серверов на различные БД.

Однако, нельзя ставить один `load balancer`, так как это угрожает общей `availability` системы и есть риск перегрузки. Используем так называемую `redundancy` - увеличение числа элементов в системе (==horizontal scaling), чтобы избежать "узких мест". Можем сделать `Active` или `Passive redundancy`. Чтобы кэш не копился в Redis и не ухудшал общую работу нашей системы, используем `Cache Eviction policy`, например, `LRU` (least recently used), и тогда кэш всегда будет в оптимальном состоянии. (PS: вместо Redis можно использовать `CDN` - content delivery network. Например, Cloudfare или Google Cloud CDN)

Возникает вопрос: а как `load balancer` будет понимать куда распределять юзеров (на какие сервера)? Есть много способов настройки `load balancer`: random/round robin/weighted round robin/на базе нагрузки (делать health checks серверов)/на основе IP address юзеров/на основе path. Если мы используем кэш (а без него никак в высоконагруженной системе), то обычный round robin не подойдет, так как он распределяет сверху-вниз по серверам и смысла от кэша не будет. Используем `hashing` (либо `consistent hashing`, либо `rendezvous hashing`). Например, хэшируем имя пользователя или его `IP address`. Таким образом, если один из серверов умирает, то клиент, именно этот один клиент, будет перенаправлен на другой сервер (в случае `consistent hashing` это будет следующий сервер расположенный на условной окружности).

Также стоит отметить о важности `replication` и `sharding` в системе. Первый поможет нам сохранить наши результаты, если БД умрет или что-то случится с системой, а также может встать на замену умершей БД и выполнять роль ведущей БД. Второй термин поможет нам оптимизировать нашу БД. Стоит отметить, что "партиционирование" стоит делать через `hashing function`, чтобы избежать `hot spots` и чтобы не было skew в данных. Решать к какому `shard` (куску) обращаться будет также load balancer, установленный после сервера.


<h3>Как защититься от накруток</h3>

Предположим для начала, что накрутка осуществляется одним юзером. В header у `http request`
находятся `credetentials` юзера (либо можно использовать что-то другое, например, уникальный никнейм). 
В данном случае схема будет похожа на противодействие `DoS` атакам. Ставим `load balancer`, который будет отправлять запрос конкретного юзера на определнный сервер/БД. В сервере/БД у нас будет кэш, который хранит время последнего обращения данного юзера. В системе ставим желаемый максимум запросов за определенный промежуток времени. Если он превышен - возвращаем ошибку. Также можно улучшить данную систему и имплементировать `rate limiting in tiers`, то есть несколько ограничений. Например, 5 запросов в 10 секунд и не более 10 ща одну минуту.

Однако, стоит сказать, что если накрутка будет осуществляться с разных машин (как происходит при `DDoS` атаках), то все становится гораздо сложнее, потому что load balancer не сможет отследить по IP address и иные методы необходимо применять.


<h3>Оценка RPS</h3>

Во-первых, стоит сказать, что можно использовать `wrk`, который нагрузит сервер по максимуму (кастомизация нагрузки присутствует). Пример нагрузки моего сервера ниже:

```bash
(base) MacBook-Air:~ daniilslobodenuk$ wrk -t12 -c400 -d30s http://0.0.0.0:8000/
Running 30s test @ http://0.0.0.0:8000/
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   766.98ms  182.46ms   1.74s    91.73%
    Req/Sec    16.88     12.59   140.00     74.40%
  5178 requests in 30.09s, 10.95MB read
  Socket errors: connect 0, read 1452, write 233, timeout 0
Requests/sec:    172.07
Transfer/sec:    372.53KB
```

Во-вторых, есть различные формулы для оценки `RPS`. Одну из них я предлагаю разобрать:
	- берем необходимое число юзеров в час  A
	- берем примерное время "пути" (от ввода юзером страницы в поисковике, до возвращения response) B
	- кол-во шагов в "пути" (например, кол-во задействованных HTTP requests) C
	- кол-во запросов в каждом шаге одного пути (а шагов может быть несколько) D
	=> (C * D * A)/(B * D)^2
